import xml.etree.ElementTree as ET
import pickle
import os
import argparse
from os import listdir, getcwd
from os.path import isfile, join as join_paths
import numpy as np
import sys
import shutil
import random 
import math

width_in_cfg_file = 416.
height_in_cfg_file = 416.

row_divs = 32
col_divs = 32

parser = argparse.ArgumentParser()


parser.add_argument(
    '-p',
    '--data_path',
    dest='data_dir',
    default='data',
    help='Path to dataset directories (directory containing datasets)',
)
parser.add_argument(
    '-d',
    '--dataset',
    dest='cust_dir',
    help='Path to dataset data (images and xml annotations)',
    required=True
)
parser.add_argument(
    '-output_dir',
    default = 'generated_anchors',
    type = str,
    help='Output anchor directory\n' 
)  
parser.add_argument(
    '-num_clusters',
    default = 5,
    type = int,
    help='number of clusters\n' 
)  


def convert_1(size, box):
    dw = 1./size[0]
    dh = 1./size[1]
    x = (box[0] + box[1])/2.0
    y = (box[2] + box[3])/2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x*dw
    w = w*dw
    y = y*dh
    h = h*dh
    return (x,y,w,h)

def convert_annotation_1(traindir, image_id, list_file):
    in_file     = open('{}/{}/{}/{}.xml'.format(data_dir,cust_dir, traindir, image_id))
    out_file    = open('{}/{}/{}_labels/{}.txt'.format(data_dir,cust_dir, traindir, image_id), 'w')
    tree=ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)

    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult) == 1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert_1((w,h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')

def convert_annotation_2(traindir, image_id, list_file):
    in_file = open('{}/{}/{}/{}.xml'.format(data_dir,cust_dir,traindir, image_id))
    tree=ET.parse(in_file)
    root = tree.getroot()

    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult)==1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (int(float(xmlbox.find('xmin').text)), int(float(xmlbox.find('ymin').text)), int(float(xmlbox.find('xmax').text)), int(float(xmlbox.find('ymax').text)))
        list_file.write(" " + ",".join([str(a) for a in b]) + ',' + str(cls_id))

def gen_image_annots(sets):
    wd = getcwd()

    for dtset, traindir in sets:
        imgs = list(filter(lambda x: '.jpg' in x, os.listdir(os.path.join(dtset,traindir)))) #Generate list of images
        image_ids = [img.split('.')[0] for img in imgs] # Strip off extension
        # image_ids = open('{}/{}.txt'.format(p1, p2)).read().strip().split()
        list_file = open('%s/%s.txt'%(dtset, traindir), 'w')
        for image_id in image_ids:
            list_file.write('{}/{}/{}/{}.jpg'.format(wd, dtset, traindir, image_id))
            convert_annotation_1(traindir, image_id, list_file)
            # list_file.write('\n')
        list_file.close()

def gen_listdir_annots(data_path, sets):
    if not os.path.exists(data_path + '.txt'):
        open(data_path+'.txt', 'w+').write('\n'.join(os.listdir(data_path)))
    wd = getcwd()
    for dtset, traindir in sets:
        imgs = list(filter(lambda x: '.jpg' in x, os.listdir(os.path.join(dtset,traindir)))) #Generate list of images
        image_ids = [img.split('.')[0] for img in imgs] # Strip off extension
        # image_ids = open('{}/{}.txt'.format(p1, p2)).read().strip().split()
        list_file_path = '%s/%s.txt'%(dtset, traindir)
        list_file = open(list_file_path, 'w')
        for image_id in image_ids:
            list_file.write('{}/{}/{}/{}.jpg'.format(wd, dtset, traindir, image_id))
            convert_annotation_2(traindir, image_id, list_file)
            list_file.write('\n')
        list_file.close()
    return list_file_path

'''
Created on Feb 20, 2017
@author: jumabek
_______________________________________________________________________________________________
Generate anchors using metadata files generated by ###
_______________________________________________________________________________________________
'''


def IOU(x,centroids):
    similarities = []
    k = len(centroids)
    for centroid in centroids:
        c_w,c_h = centroid
        w,h = x
        if c_w>=w and c_h>=h:
            similarity = w*h/(c_w*c_h)
        elif c_w>=w and c_h<=h:
            similarity = w*c_h/(w*h + (c_w-w)*c_h)
        elif c_w<=w and c_h>=h:
            similarity = c_w*h/(w*h + c_w*(c_h-h))
        else: #means both w,h are bigger than c_w and c_h respectively
            similarity = (c_w*c_h)/(w*h)
        similarities.append(similarity) # will become (k,) shape
    return np.array(similarities) 

def avg_IOU(X,centroids):
    n,d = X.shape
    sum = 0.
    for i in range(X.shape[0]):
        #note IOU() will return array which contains IoU for each centroid and X[i] // slightly ineffective, but I am too lazy
        sum+= max(IOU(X[i],centroids)) 
    return sum/n

def write_anchors_to_file(centroids,X,anchor_file):
    f = open(anchor_file,'w')
    
    anchors = centroids.copy()
    print(anchors.shape)

    for i in range(anchors.shape[0]):
        anchors[i][0]*=width_in_cfg_file/row_divs
        anchors[i][1]*=height_in_cfg_file/col_divs
         

    widths = anchors[:,0]
    sorted_indices = np.argsort(widths)

    print('Anchors = ', anchors[sorted_indices])
        
    for i in sorted_indices[:-1]:
        f.write('%0.2f,%0.2f, '%(anchors[i,0],anchors[i,1]))

    #there should not be comma after last anchor, that's why
    f.write('%0.2f,%0.2f\n'%(anchors[sorted_indices[-1:],0],anchors[sorted_indices[-1:],1]))
    
    f.write('%f\n'%(avg_IOU(X,centroids)))
    print()

def kmeans(X,centroids,eps,anchor_file):
    
    N = X.shape[0]
    iterations = 0
    k,dim = centroids.shape
    prev_assignments = np.ones(N)*(-1)    
    iter = 0
    old_D = np.zeros((N,k))

    while True:
        D = [] 
        iter+=1           
        for i in range(N):
            d = 1 - IOU(X[i],centroids)
            D.append(d)
        D = np.array(D) # D.shape = (N,k)
        
        print("iter {}: dists = {}".format(iter,np.sum(np.abs(old_D-D))))
            
        #assign samples to centroids 
        assignments = np.argmin(D,axis=1)
        
        if (assignments == prev_assignments).all() :
            print("Centroids = ",centroids)
            write_anchors_to_file(centroids,X,anchor_file)
            return

        #calculate new centroids
        centroid_sums=np.zeros((k,dim),np.float)
        for i in range(N):
            centroid_sums[assignments[i]]+=X[i]        
        for j in range(k):            
            centroids[j] = centroid_sums[j]/(np.sum(assignments==j))
        
        prev_assignments = assignments.copy()     
        old_D = D.copy()  



args = parser.parse_args()
data_dir, cust_dir, classes = None, None, None

def main(args):
    global data_dir, cust_dir, classes
    data_dir,cust_dir = args.data_dir, args.cust_dir
    data_path = join_paths(data_dir,cust_dir)

    sets=[\
        (data_path, 'data_train'), \
        ]
    
    for dtp, dir in sets:
        new = join_paths(dtp,dir+'_labels')
        os.mkdir(new) if not os.path.exists(new) else None

    # classes = ["snail"]
    classes = [line for line in open('labels.txt','r').read().split('\n')]

    gen_image_annots(sets)
    filelist = gen_listdir_annots(data_path,sets)
    
    anchor_out_path = join_paths(data_path,args.output_dir)

    if not os.path.exists(anchor_out_path):
        os.mkdir(anchor_out_path)

    f = open(filelist)
  
    lines = [line.rstrip('\n') for line in f.readlines()]
    
    annotation_dims = []

    size = np.zeros((1,1,3))
    for line in lines:
        line = line.split(' ')[0]
        #line = line.replace('images','labels')
        #line = line.replace('img1','labels')
        line = line.replace('data_train','data_train_labels')        
        

        line = line.replace('.jpg','.txt')
        line = line.replace('.png','.txt')
        print(line)
        f2 = open(line)
        for line in f2.readlines():
            line = line.rstrip('\n')
            w,h = line.split(' ')[3:]            
            #print(w,h)
            annotation_dims.append(tuple(map(float,(w,h))))
    annotation_dims = np.array(annotation_dims)
  
    eps = 0.005
    num_clusters = args.num_clusters
    if num_clusters == 0:
        for num_clusters in range(1,11): #we make 1 through 10 clusters 
            anchor_file = join_paths( data_path, args.output_dir,'anchors%d.txt'%(num_clusters))

            indices = [ random.randrange(annotation_dims.shape[0]) for i in range(num_clusters)]
            centroids = annotation_dims[indices]
            kmeans(annotation_dims,centroids,eps,anchor_file)
            print('centroids.shape', centroids.shape)
    else:
        anchor_file = join_paths( data_path, args.output_dir,'anchors%d.txt'%(args.num_clusters))
        indices = [ random.randrange(annotation_dims.shape[0]) for i in range(args.num_clusters)]
        centroids = annotation_dims[indices]
        kmeans(annotation_dims,centroids,eps,anchor_file)
        print('centroids.shape', centroids.shape)

if __name__=="__main__":
    main(args)
